{"nbformat_minor": 1, "cells": [{"source": "# Spark to DocumentDB Connector\n\nConnecting Apache Spark to Azure DocumentDB accelerates your ability to solve your fast moving Data Sciences problems where your data can be quickly persisted and retrieved using Azure DocumentDB.  With the Spark to DocumentDB conector, you can more easily solve scenarios including (but not limited to) blazing fast IoT scenarios, update-able columns when performing analytics, push-down predicate filtering, and performing advanced analytics to data sciences against your fast changing data against a geo-replicated managed document store with guaranteed SLAs for consistency, availability, low latency, and throughput.   \n\nThe Spark to DocumentDB connector utilizes the [Azure DocumentDB Java SDK](https://github.com/Azure/azure-documentdb-java) will utilize the following flow:\n\n<img style=\"align: left;\" src=\"https://raw.githubusercontent.com/dennyglee/notebooks/master/images/Azure-DocumentDB-Spark_Connector_600x266.png\">\n\n\n\nThe data flow is as follows:\n\n1. Connection is made from Spark master node to DocumentDB gateway node to obtain the partition map. Note, user only specifies Spark and DocumentDB connections, the fact that it connects to the respective master and gateway nodes is transparent to the user.\n2. This information is provided back to the Spark master node. At this point, we should be able to parse the query to determine which partitions (and their locations) within DocumentDB we need to access.\n3. This information is transmitted to the Spark worker nodes ...\n4. Thus allowing the Spark worker nodes to connect directly to the DocumentDB partitions directly to extract the data that is needed and bring the data back to the Spark partitions within the Spark worker nodes.\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure\n{ \"jars\": [\"wasb:///example/jars/azure-documentdb-1.9.6.jar\",\"wasb:///example/jars/azure-documentdb-spark-0.0.1.jar\"],\n  \"conf\": {\n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///example/jars/azure-documentdb-1.9.6.jar', u'wasb:///example/jars/azure-documentdb-spark-0.0.1.jar'], u'kind': 'spark', u'conf': {u'spark.jars.excludes': u'org.scala-lang:scala-reflect'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "// Import Time libraries\nimport org.joda.time._\nimport org.joda.time.format._", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1489125951706_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-sparki.fltxrea2z3tu3d3pwijsza32ic.gx.internal.cloudapp.net:8088/proxy/application_1489125951706_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.13:30060/node/containerlogs/container_e01_1489125951706_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nimport org.joda.time.format._"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "// Import Spark to DocumentDB Connector\nimport com.microsoft.azure.documentdb.spark.schema._\nimport com.microsoft.azure.documentdb.spark._\nimport com.microsoft.azure.documentdb.spark.config.Config\n\n// Connect to DocumentDB Database\nval readConfig2 = Config(Map(\"Endpoint\" -> \"https://doctorwho.documents.azure.com:443/\",\n\"Masterkey\" -> \"le1n99i1w5l7uvokJs3RT5ZAH8dc3ql7lx2CG0h0kK4lVWPkQnwpRLyAN0nwS1z4Cyd1lJgvGUfMWR3v8vkXKA==\",\n\"Database\" -> \"DepartureDelays\",\n\"preferredRegions\" -> \"Central US;East US 2;\",\n\"Collection\" -> \"flights_pcoll\", \n\"SamplingRatio\" -> \"1.0\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "readConfig2: com.microsoft.azure.documentdb.spark.config.Config = com.microsoft.azure.documentdb.spark.config.ConfigBuilder$$anon$1@4848afe"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "// Create collection connection \nval coll = spark.sqlContext.read.DocumentDB(readConfig2)\ncoll.createOrReplaceTempView(\"c\")", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Query 1: Flights departing from Seattle (Top 100)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "// Run, get row count, and time query\nvar query = \"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c WHERE c.origin = 'SEA' LIMIT 100\"\nval start = new DateTime()\nval df = spark.sql(query)\ndf.count()\nval end = new DateTime()\nval duration = new Duration(start, end)\n\n// Create DataFrame\ndf.createOrReplaceTempView(\"df\")\n\n// Print out duration of query\nPeriodFormat.getDefault().print(duration.toPeriod())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res30: String = 1 second and 195 milliseconds"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "%%sql\nselect * from df limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Query 2: Flights departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "// Run, get row count, and time query\nvar query = \"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c WHERE c.origin = 'SEA'\"\nval start = new DateTime()\nval df = spark.sql(query)\ndf.count()\nval end = new DateTime()\nval duration = new Duration(start, end)\n\n// Create DataFrame\ndf.createOrReplaceTempView(\"df\")\n\n// Print out duration of query\nPeriodFormat.getDefault().print(duration.toPeriod())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res39: String = 1 second and 465 milliseconds"}], "metadata": {"collapsed": false}}, {"source": "### Determine the number of flights departing from Seattle (in this dataset)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "%%sql\nselect count(1) from df", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Total delay grouped by destination\nNot just `counts` but with Spark SQL and DocumentDB, can easily do `GROUP BY`", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "%%sql\nselect destination, sum(delay) as TotalDelay from df group by destination order by sum(delay) desc limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Get distinct ordered destination airports departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "%%sql\nselect distinct destination from df order by destination limit 5", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Top 5 delayed destination cities departing from Seattle (by Total Delay)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "%%sql\nselect destination, sum(delay) \nfrom df \nwhere delay < 0 \ngroup by destination \norder by sum(delay) limit 5", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Calculate median delays by destination cities departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "%%sql\nselect destination, percentile_approx(delay, 0.5) as median_delay \nfrom df \nwhere delay < 0 \ngroup by destination \norder by percentile_approx(delay, 0.5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Query 3: Access all data (1.4M rows)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "// Run, get row count, and time query\nvar query = \"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c\"\nval start = new DateTime()\nval df = spark.sql(query)\ndf.count()\nval end = new DateTime()\nval duration = new Duration(start, end)\n\n// Create DataFrame\ndf.createOrReplaceTempView(\"df\")\n\n// Print out duration of query\nPeriodFormat.getDefault().print(duration.toPeriod())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res59: String = 10 seconds and 424 milliseconds"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "widgets": {"state": {"a7067ab096af4a3f96728d4b9bf105b0": {"views": [{"cell_index": 18}]}, "14426e87db2248e99e34915a74beaf2f": {"views": [{"cell_index": 18}]}, "b2edaf257b1043cd9c5ea8bd1b49279a": {"views": [{"cell_index": 13}]}, "78f48188b99240969d6daab275713f6f": {"views": [{"cell_index": 7}]}, "3e9e394edffd43099891807341e43537": {"views": [{"cell_index": 16}]}, "2bfd1c0d0ab64f3cb93228977136ce0e": {"views": [{"cell_index": 16}]}, "8dc79ae67d1a48469b0a0684a42dc148": {"views": [{"cell_index": 18}]}, "05ba406c6f884a428418254526f0fd82": {"views": [{"cell_index": 15}]}, "6a6262ff9540450a832793bb8808e709": {"views": [{"cell_index": 18}]}, "b08929ca71c24656a3eae395502277ac": {"views": [{"cell_index": 10}]}, "02bec5f98f6a4050bc2b58e9f12303b5": {"views": [{"cell_index": 13}]}, "a6a1b6c4e4f5481dadc659040332f7da": {"views": [{"cell_index": 15}]}, "b0edbc14c5074ad8aad95d2588e109c9": {"views": [{"cell_index": 15}]}, "03504479fcd54388aa8690a6aeedc1ef": {"views": [{"cell_index": 18}]}, "1f5cfa2623ed46e88647ad3983bb454f": {"views": [{"cell_index": 10}]}, "f90615f8504a4db8a454fdddd99364a6": {"views": [{"cell_index": 16}]}, "4811470d41d14cb4861dfa7fabaf39ef": {"views": [{"cell_index": 15}]}, "16c2d2b4fe194bcca9fa37f3e888ecf5": {"views": [{"cell_index": 10}]}, "d67b30696b2f404cb98e5f4cbef710fd": {"views": [{"cell_index": 7}]}, "57d2332abac2497bb237baecefc71038": {"views": [{"cell_index": 7}]}}, "version": "1.2.0"}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}