{"nbformat_minor": 1, "cells": [{"source": "# Spark to DocumentDB Connector\n\nConnecting Apache Spark to Azure DocumentDB accelerates your ability to solve your fast moving Data Sciences problems where your data can be quickly persisted and retrieved using Azure DocumentDB.  With the Spark to DocumentDB conector, you can more easily solve scenarios including (but not limited to) blazing fast IoT scenarios, update-able columns when performing analytics, push-down predicate filtering, and performing advanced analytics to data sciences against your fast changing data against a geo-replicated managed document store with guaranteed SLAs for consistency, availability, low latency, and throughput.   \n\nThe Spark to DocumentDB connector utilizes the [Azure DocumentDB Java SDK](https://github.com/Azure/azure-documentdb-java) will utilize the following flow:\n\n<img style=\"align: left;\" src=\"https://raw.githubusercontent.com/dennyglee/notebooks/master/images/Azure-DocumentDB-Spark_Connector_600x266.png\">\n\n\n\nThe data flow is as follows:\n\n1. Connection is made from Spark master node to DocumentDB gateway node to obtain the partition map. Note, user only specifies Spark and DocumentDB connections, the fact that it connects to the respective master and gateway nodes is transparent to the user.\n2. This information is provided back to the Spark master node. At this point, we should be able to parse the query to determine which partitions (and their locations) within DocumentDB we need to access.\n3. This information is transmitted to the Spark worker nodes ...\n4. Thus allowing the Spark worker nodes to connect directly to the DocumentDB partitions directly to extract the data that is needed and bring the data back to the Spark partitions within the Spark worker nodes.\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure\n{ \"jars\": [\"wasb:///example/jars/azure-documentdb-1.9.6.jar\",\"wasb:///example/jars/azure-documentdb-spark-0.0.1.jar\"],\n  \"conf\": {\n    \"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",   \n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///example/jars/azure-documentdb-1.9.6.jar', u'wasb:///example/jars/azure-documentdb-spark-0.0.1.jar'], u'kind': 'spark', u'conf': {u'spark.jars.packages': u'graphframes:graphframes:0.3.0-spark2.0-s_2.11', u'spark.jars.excludes': u'org.scala-lang:scala-reflect'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "// Import Spark to DocumentDB Connector\nimport com.microsoft.azure.documentdb.spark.schema._\nimport com.microsoft.azure.documentdb.spark._\nimport com.microsoft.azure.documentdb.spark.config.Config\n\n// Connect to DocumentDB Database\nval readConfig2 = Config(Map(\"Endpoint\" -> \"https://doctorwho.documents.azure.com:443/\",\n\"Masterkey\" -> \"le1n99i1w5l7uvokJs3RT5ZAH8dc3ql7lx2CG0h0kK4lVWPkQnwpRLyAN0nwS1z4Cyd1lJgvGUfMWR3v8vkXKA==\",\n\"Database\" -> \"DepartureDelays\",\n\"preferredRegions\" -> \"Central US;East US 2;\",\n\"Collection\" -> \"flights_pcoll\", \n\"SamplingRatio\" -> \"1.0\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>14</td><td>application_1489125951706_0013</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-sparki.fltxrea2z3tu3d3pwijsza32ic.gx.internal.cloudapp.net:8088/proxy/application_1489125951706_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.8:30060/node/containerlogs/container_e01_1489125951706_0013_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nreadConfig2: com.microsoft.azure.documentdb.spark.config.Config = com.microsoft.azure.documentdb.spark.config.ConfigBuilder$$anon$1@4848afe"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "// Create collection connection \nval coll = spark.sqlContext.read.DocumentDB(readConfig2)\ncoll.createOrReplaceTempView(\"c\")", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Query 1: Flights departing from Seattle (Top 100)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "// Run, get row count, and time query\nval df = spark.sql(\"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c WHERE c.origin = 'SEA' LIMIT 100\")\ndf.createOrReplaceTempView(\"df\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "%%sql\nselect * from df limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Query 2: Flights departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "// Run, get row count, and time query\nval df = spark.sql(\"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c WHERE c.origin = 'SEA'\")\ndf.createOrReplaceTempView(\"df\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Determine the number of flights departing from Seattle (in this dataset)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "%%sql\nselect count(1) from df", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Total delay grouped by destination\nNot just `counts` but with Spark SQL and DocumentDB, can easily do `GROUP BY`", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "%%sql\nselect destination, sum(delay) as TotalDelays \nfrom df \ngroup by destination \norder by sum(delay) desc limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Get distinct ordered destination airports departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "%%sql\nselect distinct destination from df order by destination limit 5", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Top 5 delayed destination cities departing from Seattle (by Total Delay)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "%%sql\nselect destination, sum(delay) \nfrom df \nwhere delay < 0 \ngroup by destination \norder by sum(delay) limit 5", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Calculate median delays by destination cities departing from Seattle", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "%%sql\nselect destination, percentile_approx(delay, 0.5) as median_delay \nfrom df \nwhere delay < 0 \ngroup by destination \norder by percentile_approx(delay, 0.5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Query 3: Access all data (~1.39M rows) or Access data from key airports (e.g. SEA, SFO, SJC, JFK, ATL, etc.)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "// Run, get row count, and time query (filtering out test data)\nval departureDelays = spark.sql(\"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c WHERE c.origin IN ('SEA', 'SFO', 'SJC', 'JFK', 'LAX', 'LAS', 'BOS', 'IAD', 'ORD', 'DFW', 'MSP', 'DTW', 'DEN', 'ATL')\")\ndepartureDelays.createOrReplaceTempView(\"departureDelays\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "// Checking the number of rows (and cache)\ndepartureDelays.cache()\ndepartureDelays.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res10: Long = 547350"}], "metadata": {"collapsed": false}}, {"source": "## Using GraphFrames for Apache Spark to run motif queries\nUsing GraphFrames to build a graph against the flights data stored within DocumentDB.\n\n### Prepare the data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "// Set File Paths\nval airportsnaFilePath = \"wasb://data@doctorwhostore.blob.core.windows.net/airport-codes-na.txt\"\n\n// Obtain airport information\nval airportsna = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"delimiter\", \"\\t\").load(airportsnaFilePath)\nairportsna.createOrReplaceTempView(\"airports_na\")\n\n// Available IATA codes from the departuredelays sample dataset\nval tripIATA = spark.sql(\"select distinct iata from (select distinct origin as iata from departureDelays union all select distinct destination as iata from departureDelays) a\")\ntripIATA.createOrReplaceTempView(\"tripIATA\")\n\n// Only include airports with atleast one trip from the departureDelays dataset\nval airports = spark.sql(\"select f.IATA, f.City, f.State, f.Country from airports_na f join tripIATA t on t.IATA = f.IATA\")\nairports.createOrReplaceTempView(\"airports\")\nairports.cache()\n\n// Build `departureDelays_geo` DataFrame\nval departureDelays_geo = spark.sql(\"select cast(f.date as int) as tripid, cast(concat(concat(concat(concat(concat(concat('2014-', concat(concat(substr(cast(f.date as string), 1, 2), '-')), substr(cast(f.date as string), 3, 2)), ' '), substr(cast(f.date as string), 5, 2)), ':'), substr(cast(f.date as string), 7, 2)), ':00') as timestamp) as `localdate`, cast(f.delay as int), cast(f.distance as int), f.origin as src, f.destination as dst, o.city as city_src, d.city as city_dst, o.state as state_src, d.state as state_dst from departuredelays f join airports o on o.iata = f.origin join airports d on d.iata = f.destination\") \n\n// Create Temporary View and cache\ndepartureDelays_geo.createOrReplaceTempView(\"departureDelays_geo\")\ndepartureDelays_geo.cache()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res27: departureDelays_geo.type = [tripid: int, localdate: timestamp ... 8 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "// Check number of flights\ndepartureDelays_geo.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res29: Long = 541524"}], "metadata": {"collapsed": false}}, {"source": "### Build the graphFrame", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "// import graphframes package\nimport org.graphframes._\n\n// Create Vertices (airports) and Edges (flights)\nval tripVertices = airports.withColumnRenamed(\"IATA\", \"id\").distinct()\nval tripEdges = departureDelays_geo.select(\"tripid\", \"delay\", \"src\", \"dst\", \"city_dst\", \"state_dst\")\n\n// Cache Vertices and Edges\ntripEdges.cache()\ntripVertices.cache()\n\n// Build tripGraph GraphFrame\n//   This GraphFrame builds up on the vertices and edges based on our trips (flights)\nval tripGraph = GraphFrame(tripVertices, tripEdges)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "tripGraph: org.graphframes.GraphFrame = GraphFrame(v:[id: string, City: string ... 2 more fields], e:[src: string, dst: string ... 4 more fields])"}], "metadata": {"collapsed": false}}, {"source": "### Determine the number of airports and trips", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "// Number of airports\ntripGraph.vertices.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res41: Long = 262"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "// Number of flights\ntripGraph.edges.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res43: Long = 541524"}], "metadata": {"collapsed": false}}, {"source": "### What flights departing SEA are most likely to have significant delays", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "val flightDelays = tripGraph.edges.filter(\"src = 'SEA' and delay > 0\").groupBy(\"src\", \"dst\").avg(\"delay\").sort(desc(\"avg(delay)\"))\nflightDelays.createOrReplaceTempView(\"flightDelays\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "%%sql\nselect * from flightDelays order by `avg(delay)` desc limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Which is the most important airport (in terms of connections)\nNote this is from this *filtered* dataset", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "val airportConnections = tripGraph.degrees.sort(desc(\"degree\"))\nairportConnections.createOrReplaceTempView(\"airportConnections\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": "%%sql\nselect id, degree from airportConnections order by degree desc limit 10", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Direct flights between Seattle and San Jose?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "val filteredPaths = tripGraph.bfs.fromExpr(\"id = 'SEA'\").toExpr(\"id = 'SJC'\").maxPathLength(1).run()\nfilteredPaths.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------------+\n|                from|                  e0|                  to|\n+--------------------+--------------------+--------------------+\n|[SEA,Seattle,WA,USA]|[1010600,-2,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1012030,-4,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1011215,-6,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1011855,-3,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1010710,-1,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1020600,2,SEA,SJ...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1022030,-3,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1021600,-2,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1021215,-9,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1021855,-1,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1020710,-9,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1030600,-5,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1032030,-1,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1031600,-7,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1031215,-3,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1031855,-1,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1030710,-5,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1040600,4,SEA,SJ...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1042030,-2,SEA,S...|[SJC,San Jose,CA,...|\n|[SEA,Seattle,WA,USA]|[1041215,-4,SEA,S...|[SJC,San Jose,CA,...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "### Direct flights between Buffalo and San Jose?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "val filteredPaths = tripGraph.bfs.fromExpr(\"id = 'SJC'\").toExpr(\"id = 'BUF'\").maxPathLength(1).run()\nfilteredPaths.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+----+-----+-------+\n| id|City|State|Country|\n+---+----+-----+-------+\n+---+----+-----+-------+"}], "metadata": {"collapsed": false}}, {"source": "No direct flights, but how about one layover?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "val filteredPaths = tripGraph.bfs.fromExpr(\"id = 'SJC'\").toExpr(\"id = 'BUF'\").maxPathLength(2).run()\nfilteredPaths.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+-------------------+--------------------+--------------------+\n|                from|                  e0|                 v1|                  e1|                  to|\n+--------------------+--------------------+-------------------+--------------------+--------------------+\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1010635,-6,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1011059,13,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1011427,19,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1020635,-4,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1021059,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1021427,194,BOS,...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1030635,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1031059,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1031427,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1040635,16,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1041552,96,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1050635,1,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1051059,48,BOS,B...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1051427,443,BOS,...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1060635,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1061059,294,BOS,...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1061427,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1070730,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1071730,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n|[SJC,San Jose,CA,...|[1012124,16,SJC,B...|[BOS,Boston,MA,USA]|[1080710,0,BOS,BU...|[BUF,Buffalo,NY,USA]|\n+--------------------+--------------------+-------------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "### What is the most common transfer point?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "filteredPaths.groupBy(\"v1.id\", \"v1.City\").count().orderBy(desc(\"count\")).limit(10).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+-----------+------+\n| id|       City| count|\n+---+-----------+------+\n|LAS|  Las Vegas|107442|\n|ORD|    Chicago| 87696|\n|JFK|   New York| 31968|\n|ATL|    Atlanta| 28910|\n|BOS|     Boston|  1488|\n|MSP|Minneapolis|   164|\n+---+-----------+------+"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "widgets": {"state": {"e78ff9ae167a494fa8d21260d4b8282c": {"views": [{"cell_index": 35}]}, "4a0c279995c340fd9a03ba2d6bf5bdef": {"views": [{"cell_index": 35}]}, "3ea1dd0ba35c40719a1073cb6294381a": {"views": [{"cell_index": 23}]}, "05e60f666acb4837ae9c013f0595d1dc": {"views": [{"cell_index": 14}]}, "54226246874a4db2a582849078ed913c": {"views": [{"cell_index": 16}]}, "2e031b1607fd4392bc585a37f76e7299": {"views": [{"cell_index": 6}]}, "f2d14706598b4b70a035d3942638762e": {"views": [{"cell_index": 12}]}, "bb51dfd096314b809ebcfde0ea5cac17": {"views": [{"cell_index": 18}]}, "af6b8e9187574705941acedd064928a5": {"views": [{"cell_index": 16}]}, "b41c3210a2914a1693c252ac88f7f21d": {"views": [{"cell_index": 14}]}, "9857436db414464ea20d69c714e1f7d8": {"views": [{"cell_index": 6}]}, "de3a7303393d432eb9d3968869118d79": {"views": [{"cell_index": 18}]}, "1be8a3cbf3494dcd9318cc2f5217af7b": {"views": [{"cell_index": 32}]}, "634fd5da540e47b897885555063cc2c2": {"views": [{"cell_index": 32}]}, "a7cabfeafbfb47968753c5dcfb7dbd2e": {"views": [{"cell_index": 10}]}, "c3e25dcf8b44400eabc79b491a6bf73d": {"views": [{"cell_index": 10}]}, "ba861970520c46aaa33588226123af61": {"views": [{"cell_index": 35}]}, "ee7bcac9ddac4e20a33bb3df7cecc753": {"views": [{"cell_index": 35}]}, "039072fc892a45c585423f1c04336760": {"views": [{"cell_index": 12}]}, "952f43d97c7d480ea8c471e75f91180b": {"views": [{"cell_index": 35}]}}, "version": "1.2.0"}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}